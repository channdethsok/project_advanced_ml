{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from copy import deepcopy\n",
    "from symusic import Score\n",
    "from miditok import TSD, TokenizerConfig\n",
    "\n",
    "# Random MIDI pitch augmentation\n",
    "def randomize_midi_pitch(midi_score, prob=0.2, max_change=4):\n",
    "    new_score = deepcopy(midi_score)\n",
    "    for track in new_score.tracks:\n",
    "        for note in track.notes:\n",
    "            if random.random() < prob:\n",
    "                change = random.randint(-max_change, max_change)\n",
    "                note.pitch = max(0, min(note.pitch + change, 127))\n",
    "    return new_score\n",
    "\n",
    "# Dataset class\n",
    "class LyricsMidiDataset(Dataset):\n",
    "    def __init__(self, dataframe, lyrics_tokenizer, midi_tokenizer, max_length, root_dir=None, augment=False):\n",
    "        self.dataframe = dataframe\n",
    "        self.lyrics_tokenizer = lyrics_tokenizer\n",
    "        self.midi_tokenizer = midi_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.augment = augment\n",
    "        self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        lyrics = self.dataframe.iloc[idx]['lyrics']\n",
    "        midi_path = self.dataframe.iloc[idx]['midi_path']\n",
    "        if self.root_dir:\n",
    "            midi_path = os.path.join(self.root_dir, midi_path)\n",
    "\n",
    "        midi_path = os.path.normpath(midi_path)\n",
    "        if not os.path.isfile(midi_path):\n",
    "            raise FileNotFoundError(f\"MIDI file not found: {midi_path}\")\n",
    "\n",
    "        lyrics_tokens = self.lyrics_tokenizer(\n",
    "            lyrics + self.lyrics_tokenizer.eos_token,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Ensure padding tokens are correctly replaced for GPT-2 compatibility\n",
    "        lyrics_tokens['input_ids'][lyrics_tokens['input_ids'] == self.lyrics_tokenizer.pad_token_id] = 50256\n",
    "\n",
    "\n",
    "        midi_score = Score(midi_path)\n",
    "        if self.augment:\n",
    "            midi_score = randomize_midi_pitch(midi_score)\n",
    "\n",
    "        midi_tokens = self.midi_tokenizer.encode(midi_score)[0].ids\n",
    "        midi_tokens = self._pad_or_truncate(midi_tokens)\n",
    "        # # Debugging checks\n",
    "        # print(\"Lyrics:\", lyrics)\n",
    "        # print(\"Lyrics Tokens:\", lyrics_tokens['input_ids'])\n",
    "        # print(\"MIDI Path:\", midi_path)\n",
    "        # print(\"MIDI Tokens before padding:\", midi_tokens)\n",
    "\n",
    "        return {\n",
    "            'lyrics_ids': lyrics_tokens['input_ids'].squeeze(0),\n",
    "            'lyrics_attention_mask': lyrics_tokens['attention_mask'].squeeze(0),\n",
    "            'midi_tokens': midi_tokens\n",
    "        }\n",
    "\n",
    "    def _pad_or_truncate(self, tokens):\n",
    "        # Truncate sequences longer than `max_length`\n",
    "        if len(tokens) > self.max_length:\n",
    "            tokens = tokens[:self.max_length]\n",
    "        # Pad sequences shorter than `max_length`\n",
    "        elif len(tokens) < self.max_length:\n",
    "            tokens = tokens + [0] * (self.max_length - len(tokens))\n",
    "        return torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, GPT2LMHeadModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class LyricsGenerator(nn.Module):\n",
    "    def __init__(self, lyrics_vocab_size, d_model, max_lyrics_length, max_midi_length):\n",
    "        super(LyricsGenerator, self).__init__()\n",
    "\n",
    "        # MIDI Encoder\n",
    "        self.midi_encoder = AutoModel.from_pretrained(\"ruru2701/musicbert-v1.1\")\n",
    "        self.midi_projection = nn.Linear(self.midi_encoder.config.hidden_size, d_model)\n",
    "        self.midi_positional_embedding = nn.Embedding(max_midi_length, d_model)\n",
    "\n",
    "        # GPT-2 for lyrics\n",
    "        self.gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=50256)\n",
    "        self.gpt2.resize_token_embeddings(lyrics_vocab_size)\n",
    "        self.lyrics_positional_embedding = nn.Embedding(max_lyrics_length, d_model)\n",
    "\n",
    "        # Cross-Attention Layer\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=8)\n",
    "\n",
    "    def forward(self, lyrics_ids, lyrics_attention_mask, midi_tokens):\n",
    "        # MIDI Encoding\n",
    "        midi_outputs = self.midi_encoder(input_ids=midi_tokens)\n",
    "        midi_embeds = self.midi_projection(midi_outputs.last_hidden_state)\n",
    "        midi_positions = torch.arange(midi_tokens.size(1), device=midi_tokens.device).unsqueeze(0)\n",
    "        midi_embeds += self.midi_positional_embedding(midi_positions)\n",
    "\n",
    "        # Lyrics Encoding\n",
    "        lyrics_positions = torch.arange(lyrics_ids.size(1), device=lyrics_ids.device).unsqueeze(0)\n",
    "        lyrics_embeds = self.gpt2.transformer.wte(lyrics_ids) + self.lyrics_positional_embedding(lyrics_positions)\n",
    "\n",
    "        # Cross-Attention\n",
    "        midi_embeds_t = midi_embeds.transpose(0, 1)\n",
    "        lyrics_embeds_t = lyrics_embeds.transpose(0, 1)\n",
    "        cross_attn_output, _ = self.cross_attention(query=lyrics_embeds_t, key=midi_embeds_t, value=midi_embeds_t)\n",
    "        combined_embeds = lyrics_embeds + cross_attn_output.transpose(0, 1)\n",
    "\n",
    "        # Concatenate and Pass through GPT-2\n",
    "        combined_attention_mask = torch.cat(\n",
    "            [torch.ones((lyrics_ids.size(0), midi_tokens.size(1)), device=lyrics_ids.device), lyrics_attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "        combined_embeds = torch.cat((midi_embeds, combined_embeds), dim=1)\n",
    "        outputs = self.gpt2(inputs_embeds=combined_embeds, attention_mask=combined_attention_mask)\n",
    "        return outputs.logits[:, midi_tokens.size(1):, :]\n",
    "\n",
    "# class LyricsGenerator(nn.Module):\n",
    "#     def __init__(self, lyrics_vocab_size, d_model, max_lyrics_length, max_midi_length):\n",
    "#         super(LyricsGenerator, self).__init__()\n",
    "\n",
    "#         # MIDI Encoder\n",
    "#         self.midi_encoder = AutoModel.from_pretrained(\"ruru2701/musicbert-v1.1\")\n",
    "#         self.midi_projection = nn.Linear(self.midi_encoder.config.hidden_size, d_model)\n",
    "\n",
    "#         # GPT-2 for lyrics\n",
    "#         self.gpt2 = GPT2LMHeadModel.from_pretrained('gpt2', pad_token_id=50256)\n",
    "#         self.gpt2.resize_token_embeddings(lyrics_vocab_size)\n",
    "#         self.lyrics_positional_embedding = nn.Embedding(max_lyrics_length, d_model)\n",
    "\n",
    "#     def forward(self, lyrics_ids, lyrics_attention_mask, midi_tokens):\n",
    "#         # MIDI Encoding\n",
    "#         midi_outputs = self.midi_encoder(input_ids=midi_tokens)\n",
    "#         midi_embeds = self.midi_projection(midi_outputs.last_hidden_state)\n",
    "\n",
    "#         # Lyrics Encoding\n",
    "#         lyrics_positions = torch.arange(lyrics_ids.size(1), device=lyrics_ids.device).unsqueeze(0)\n",
    "#         lyrics_embeds = self.gpt2.transformer.wte(lyrics_ids) + self.lyrics_positional_embedding(lyrics_positions)\n",
    "\n",
    "#         # Concatenate MIDI and Lyrics Embeddings\n",
    "#         combined_embeds = torch.cat((midi_embeds, lyrics_embeds), dim=1)\n",
    "\n",
    "#         # Extend attention mask for MIDI tokens\n",
    "#         combined_attention_mask = torch.cat(\n",
    "#             [torch.ones((lyrics_ids.size(0), midi_tokens.size(1)), device=lyrics_ids.device), lyrics_attention_mask],\n",
    "#             dim=1\n",
    "#         )\n",
    "\n",
    "#         # Pass concatenated embeddings through GPT-2\n",
    "#         outputs = self.gpt2(inputs_embeds=combined_embeds, attention_mask=combined_attention_mask)\n",
    "#         return outputs.logits[:, midi_tokens.size(1):, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.amp import autocast, GradScaler\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, scheduler, epochs, device, lyrics_tokenizer):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    loss_fct = nn.CrossEntropyLoss(ignore_index=50256)\n",
    "    save_dir = \"model_checkpoint\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        loop = tqdm(train_dataloader, leave=True)\n",
    "        for i, batch in enumerate(loop):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            lyrics_ids = batch['lyrics_ids'].to(device)\n",
    "            lyrics_attention_mask = batch['lyrics_attention_mask'].to(device)\n",
    "            midi_tokens = batch['midi_tokens'].to(device)\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                logits = model(lyrics_ids, lyrics_attention_mask, midi_tokens)\n",
    "                loss = loss_fct(logits.transpose(1, 2), lyrics_ids)\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # # Debugging \n",
    "            # if i % 300 == 0:\n",
    "            #     print(f\"Epoch {epoch + 1}, Batch {i}/{len(train_dataloader)}\")\n",
    "            #     print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "            #     # Decode input lyrics and predictions\n",
    "            #     decoded_input = lyrics_tokenizer.decode(lyrics_ids[0].tolist(), skip_special_tokens=True)\n",
    "            #     predicted_tokens = logits.argmax(dim=-1)[0]\n",
    "            #     decoded_prediction = lyrics_tokenizer.decode(predicted_tokens.tolist(), skip_special_tokens=True)\n",
    "\n",
    "            #     # print(f\"Input Lyrics: {decoded_input}\")\n",
    "            #     print(\"_______________________________\")\n",
    "            #     print(f\"Predicted Lyrics: {decoded_prediction}\")\n",
    "            #     # print(f\"MIDI Tokens: {midi_tokens[0].cpu().numpy()}\")\n",
    "            loop.set_description(f\"Epoch {epoch}\")\n",
    "            loop.set_postfix(loss=loss.item())\n",
    "\n",
    "        val_loss = validate(model, val_dataloader, loss_fct, device)\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {train_loss / len(train_dataloader):.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save Checkpoint\n",
    "        save_checkpoint(model, optimizer, scheduler, epoch, save_dir)\n",
    "        print(f\"Checkpoint saved for epoch {epoch + 1}!\")\n",
    "\n",
    "def validate(model, dataloader, loss_fct, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            lyrics_ids = batch['lyrics_ids'].to(device)\n",
    "            lyrics_attention_mask = batch['lyrics_attention_mask'].to(device)\n",
    "            midi_tokens = batch['midi_tokens'].to(device)\n",
    "\n",
    "            with autocast(device_type=device.type):\n",
    "                logits = model(lyrics_ids, lyrics_attention_mask, midi_tokens)\n",
    "                loss = loss_fct(logits.transpose(1, 2), lyrics_ids)\n",
    "                val_loss += loss.item()\n",
    "    return val_loss / len(dataloader)\n",
    "\n",
    "def save_checkpoint(model, optimizer, scheduler, epoch, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch + 1}.pt\")\n",
    "    checkpoint = {\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()\n",
    "    }\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(model, optimizer, scheduler, path, device):\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Checkpoint loaded. Resuming from epoch {epoch}.\")\n",
    "    return model, optimizer, scheduler, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/lyrics_midi_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "  0%|          | 0/25 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "Epoch 0: 100%|██████████| 25/25 [00:38<00:00,  1.55s/it, loss=6.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Training Loss: 7.1577, Validation Loss: 6.8876\n",
      "Checkpoint saved for epoch 1!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 25/25 [00:38<00:00,  1.55s/it, loss=6.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Training Loss: 6.5588, Validation Loss: 6.4437\n",
      "Checkpoint saved for epoch 2!\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from pathlib import Path\n",
    "from miditok import TSD, TokenizerConfig\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "# Tokenizers\n",
    "lyrics_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lyrics_tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "\n",
    "# Load MIDI Tokenizer\n",
    "config = TokenizerConfig(\n",
    "    num_velocities=1,\n",
    "    use_chords=False,\n",
    "    use_rests=False,\n",
    "    use_tempos=False,\n",
    "    use_time_signatures=False,\n",
    ")\n",
    "midi_tokenizer = TSD(config)\n",
    "midi_tokenizer = midi_tokenizer.from_pretrained(Path(\"tokenizer\", \"tokenizer.json\"))\n",
    "\n",
    "# Dataset and Dataloader\n",
    "dataset = LyricsMidiDataset(df, lyrics_tokenizer, midi_tokenizer, max_length=512, root_dir='data', augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# val_dataloader = DataLoader(val_dataset, batch_size=4)\n",
    "\n",
    "# Define a small subset size\n",
    "subset_size = 100  # Adjust the size as needed\n",
    "train_subset = Subset(train_dataset, range(subset_size))\n",
    "val_subset = Subset(val_dataset, range(subset_size))\n",
    "\n",
    "# Create DataLoaders for the subsets\n",
    "train_dataloader = DataLoader(train_subset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_subset, batch_size=4)\n",
    "\n",
    "# Model\n",
    "model = LyricsGenerator(lyrics_vocab_size=len(lyrics_tokenizer), d_model=768, max_lyrics_length=512, max_midi_length=512)\n",
    "\n",
    "# Optimizer and Scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "total_steps = len(train_dataloader) * 10\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Train\n",
    "train(model, train_dataloader, val_dataloader, optimizer, scheduler, epochs=2, device=device, lyrics_tokenizer=lyrics_tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(\n",
    "    model, \n",
    "    midi_path, \n",
    "    lyrics_tokenizer, \n",
    "    midi_tokenizer, \n",
    "    max_midi_length, \n",
    "    max_lyrics_length, \n",
    "    num_beams=5, \n",
    "    input_text=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates lyrics conditioned on MIDI input and optional input text using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained LyricsGenerator model.\n",
    "        midi_path: Path to the MIDI file.\n",
    "        lyrics_tokenizer: Tokenizer for lyrics (e.g., GPT-2 tokenizer).\n",
    "        midi_tokenizer: Tokenizer for MIDI (e.g., miditok TSD tokenizer).\n",
    "        max_midi_length: Maximum length of MIDI token sequence.\n",
    "        max_lyrics_length: Maximum length of lyrics sequence.\n",
    "        num_beams: Number of beams for beam search.\n",
    "        input_text: Optional input text to condition lyrics generation.\n",
    "    \n",
    "    Returns:\n",
    "        Generated lyrics as a string.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize MIDI\n",
    "    try:\n",
    "        midi_score = Score(midi_path)  # Load MIDI file into Score object\n",
    "        midi_tokens = midi_tokenizer.encode(midi_score)[0].ids  # Tokenize MIDI\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing MIDI file: {e}\")\n",
    "    \n",
    "    # Pad or truncate MIDI tokens to max_midi_length\n",
    "    midi_tokens = midi_tokens[:max_midi_length]\n",
    "    padding_length = max_midi_length - len(midi_tokens)\n",
    "    midi_tokens = midi_tokens + [0] * padding_length  # Pad with 0s\n",
    "    midi_tokens = torch.tensor(midi_tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # Initialize input for lyrics generation\n",
    "    if input_text:\n",
    "        # Tokenize\n",
    "        input_ids = lyrics_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    else:\n",
    "        # Default to starting token if no input text\n",
    "        input_ids = torch.tensor(lyrics_tokenizer.encode(\"<|endoftext|>\")).unsqueeze(0).to(device)\n",
    "    \n",
    "    attention_mask = torch.ones_like(input_ids).to(device)\n",
    "\n",
    "    # Generate with beam search\n",
    "    beam_output = model.gpt2.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        max_length=max_lyrics_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=lyrics_tokenizer.pad_token_id,\n",
    "        do_sample=True\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    generated_lyrics = lyrics_tokenizer.decode(beam_output[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_lyrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Lyrics:\n",
      "It's been a while since I've been able to get my hands on one of these, so I thought I'd share it with you.\n",
      "\n",
      "I've been doing this for a while now, and it's been really fun.\n",
      "\n",
      "I have a few things I want to share with you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n",
      "\n",
      "I want you to know that I love you.\n"
     ]
    }
   ],
   "source": [
    "base_dir = 'data/'\n",
    "midi_path = os.path.join(base_dir, df[\"midi_path\"][9001])\n",
    "input_text = None\n",
    "generated_lyrics = generate_lyrics(\n",
    "    model=model,\n",
    "    midi_path=midi_path,\n",
    "    lyrics_tokenizer=lyrics_tokenizer,\n",
    "    midi_tokenizer=midi_tokenizer,\n",
    "    max_midi_length=256,\n",
    "    max_lyrics_length=512,\n",
    "    num_beams=5,\n",
    "    input_text=input_text\n",
    ")\n",
    "print(\"Generated Lyrics:\")\n",
    "print(generated_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
