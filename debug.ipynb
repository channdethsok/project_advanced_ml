{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import GPT2Tokenizer, get_linear_schedule_with_warmup\n",
    "import random\n",
    "import numpy as np\n",
    "# For reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyrics</th>\n",
       "      <th>midi_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In sleep he sang to me\\nin dreams he came\\ntha...</td>\n",
       "      <td>lmd-full_and_reddit_MIDI_dataset/sentenceWord_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I have plans and schemes\\nAnd I have hopes and...</td>\n",
       "      <td>lmd-full_and_reddit_MIDI_dataset/sentenceWord_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I get up and nothing gets me You got\\nit tough...</td>\n",
       "      <td>lmd-full_and_reddit_MIDI_dataset/sentenceWord_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man a hot like seven inches\\nfrom the midday I...</td>\n",
       "      <td>lmd-full_and_reddit_MIDI_dataset/sentenceWord_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We come from the land of the ice and snow\\nfro...</td>\n",
       "      <td>lmd-full_and_reddit_MIDI_dataset/sentenceWord_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lyrics  \\\n",
       "0  In sleep he sang to me\\nin dreams he came\\ntha...   \n",
       "1  I have plans and schemes\\nAnd I have hopes and...   \n",
       "2  I get up and nothing gets me You got\\nit tough...   \n",
       "3  Man a hot like seven inches\\nfrom the midday I...   \n",
       "4  We come from the land of the ice and snow\\nfro...   \n",
       "\n",
       "                                           midi_path  \n",
       "0  lmd-full_and_reddit_MIDI_dataset/sentenceWord_...  \n",
       "1  lmd-full_and_reddit_MIDI_dataset/sentenceWord_...  \n",
       "2  lmd-full_and_reddit_MIDI_dataset/sentenceWord_...  \n",
       "3  lmd-full_and_reddit_MIDI_dataset/sentenceWord_...  \n",
       "4  lmd-full_and_reddit_MIDI_dataset/sentenceWord_...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/lyrics_midi_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from pathlib import Path\n",
    "from miditok import TSD, TokenizerConfig\n",
    "\n",
    "# Load MIDI Tokenizer\n",
    "config = TokenizerConfig(\n",
    "    use_velocities=False,\n",
    "    use_chords=False,\n",
    "    use_rests=False,\n",
    "    use_tempos=False,\n",
    "    use_time_signatures=False,\n",
    ")\n",
    "\n",
    "midi_tokenizer = TSD(config)\n",
    "midi_tokenizer\n",
    "\n",
    "# Load GPT-2 tokenizer\n",
    "lyrics_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lyrics_tokenizer.add_special_tokens({'pad_token': '<|pad|>'})\n",
    "lyrics_vocab_size = len(lyrics_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data import randomize_midi_pitch, LyricsMidiDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = LyricsMidiDataset(df, lyrics_tokenizer, midi_tokenizer, max_length=512, root_dir='data', augment=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from src.model import LyricsGenerator\n",
    "from src.utils import train, validate, save_checkpoint, load_checkpoint\n",
    "# Instantiate model\n",
    "model = LyricsGenerator(lyrics_tokenizer=lyrics_tokenizer, d_model=768, max_lyrics_length=512, max_midi_length=512)\n",
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2236 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   7%|â–‹         | 154/2236 [02:40<36:10,  1.04s/it, loss=5.63]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m total_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m epochs\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_linear_schedule_with_warmup(optimizer, num_warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, num_training_steps\u001b[38;5;241m=\u001b[39mtotal_steps)\n\u001b[0;32m----> 7\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlyrics_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlyrics_tokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmidi_tokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmidi_tokenizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/project_advanced_ml/src/utils.py:65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, scheduler, epochs, device, lyrics_tokenizer, midi_tokenizer, save_every)\u001b[0m\n\u001b[1;32m     63\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     64\u001b[0m loop \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(loop):\n\u001b[1;32m     66\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     68\u001b[0m     lyrics_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlyrics_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/work/project_advanced_ml/src/data.py:73\u001b[0m, in \u001b[0;36mLyricsMidiDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMIDI file not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmidi_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m lyrics_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlyrics_tokenizer(\n\u001b[1;32m     66\u001b[0m     lyrics \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlyrics_tokenizer\u001b[38;5;241m.\u001b[39meos_token,\n\u001b[1;32m     67\u001b[0m     max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m midi_score \u001b[38;5;241m=\u001b[39m \u001b[43mScore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmidi_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment:\n\u001b[1;32m     75\u001b[0m     midi_score \u001b[38;5;241m=\u001b[39m randomize_midi_pitch(midi_score)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/symusic/factory.py:482\u001b[0m, in \u001b[0;36mScoreFactory.__call__\u001b[0;34m(self, x, ttype, fmt)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    477\u001b[0m     x: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m Path \u001b[38;5;241m|\u001b[39m smt\u001b[38;5;241m.\u001b[39mScore \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m960\u001b[39m,\n\u001b[1;32m    478\u001b[0m     ttype: smt\u001b[38;5;241m.\u001b[39mGeneralTimeUnit \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtick\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    479\u001b[0m     fmt: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    480\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m smt\u001b[38;5;241m.\u001b[39mScore:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mstr\u001b[39m, Path)):\n\u001b[0;32m--> 482\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mttype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_tpq(x, ttype)\n",
      "File \u001b[0;32m~/work/.venv/lib/python3.10/site-packages/symusic/factory.py:500\u001b[0m, in \u001b[0;36mScoreFactory.from_file\u001b[0;34m(self, path, ttype, fmt)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(_ \u001b[38;5;241m:=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__core_classes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mttype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 6\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "train(model, train_dataloader, val_dataloader, optimizer, scheduler, epochs, device=device, lyrics_tokenizer=lyrics_tokenizer, save_every=3, midi_tokenizer=midi_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "usage: ipykernel_launcher.py [-h] [--data_dir DATA_DIR]\n",
      "                             [--checkpoint CHECKPOINT]\n",
      "                             [--midi_index MIDI_INDEX]\n",
      "                             [--max_midi_length MAX_MIDI_LENGTH]\n",
      "                             [--max_lyrics_length MAX_LYRICS_LENGTH]\n",
      "                             [--max_length MAX_LENGTH] [--num_beams NUM_BEAMS]\n",
      "                             [--input_text INPUT_TEXT]\n",
      "                             [--num_warmup_steps NUM_WARMUP_STEPS]\n",
      "                             [--batch_size BATCH_SIZE] [--epochs EPOCHS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/onyxia/.local/share/jupyter/runtime/kernel-v34f03e6a62b6dbdb6d79564c77063eab6e9113c35.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/onyxia/work/.venv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from src.model import LyricsGenerator\n",
    "from src.utils import initialize_lyrics_tokenizer, initialize_midi_tokenizer, generate_lyrics, load_checkpoint\n",
    "from src.data import prepare_dataloaders\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Generate lyrics conditioned on MIDI input.\")\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=\"data\", help=\"Path to the lyrics_midi_dataset directory\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default=\"model_checkpoint/best_checkpoint.pth\", help=\"Path to the trained model checkpoint\")\n",
    "    parser.add_argument(\"--midi_index\", type=int, default=0, help=\"Index of the MIDI file in the dataset\")\n",
    "    parser.add_argument(\"--max_midi_length\", type=int, default=512, help=\"Maximum length of MIDI token sequence\")\n",
    "    parser.add_argument(\"--max_lyrics_length\", type=int, default=512, help=\"Maximum length of lyrics sequence\")\n",
    "    parser.add_argument(\"--max_length\", type=int, default=512,\n",
    "                        help=\"Maximum sequence length for tokenization\")\n",
    "    parser.add_argument(\"--num_beams\", type=int, default=5, help=\"Number of beams for beam search\")\n",
    "    parser.add_argument(\"--input_text\", type=str, default=None, help=\"Optional input text to condition lyrics generation\")\n",
    "    parser.add_argument(\"--num_warmup_steps\", type=int, default=0,\n",
    "                        help=\"number of warm up step\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4,\n",
    "                        help=\"Batch size for training and validation\")\n",
    "    parser.add_argument(\"--epochs\", type=int, default=10,\n",
    "                        help=\"Number of training epochs\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    dataset_path = os.path.join(args.data_dir, \"lyrics_midi_data.csv\")\n",
    "    if not os.path.isfile(dataset_path):\n",
    "        raise FileNotFoundError(f\"Dataset file not found at {dataset_path}\")\n",
    "    df = pd.read_csv(dataset_path)\n",
    "\n",
    "    if args.midi_index >= len(df):\n",
    "        raise IndexError(f\"MIDI index {args.midi_index} is out of range. Dataset has {len(df)} entries.\")\n",
    "\n",
    "    midi_path = os.path.join(args.data_dir, df[\"midi_path\"][args.midi_index])\n",
    "\n",
    "    # tokenizers\n",
    "    lyrics_tokenizer = initialize_lyrics_tokenizer()\n",
    "    midi_tokenizer = initialize_midi_tokenizer()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_dataloader, val_dataloader = prepare_dataloaders(\n",
    "        df=df,\n",
    "        lyrics_tokenizer=lyrics_tokenizer,\n",
    "        midi_tokenizer=midi_tokenizer,\n",
    "        max_length=args.max_length,\n",
    "        root_dir=args.data_dir,\n",
    "        batch_size=args.batch_size,\n",
    "        )\n",
    "    # model\n",
    "    model = LyricsGenerator(\n",
    "        lyrics_tokenizer=lyrics_tokenizer,\n",
    "        d_model=768,\n",
    "        max_lyrics_length=args.max_lyrics_length,\n",
    "        max_midi_length=args.max_midi_length,\n",
    "    )\n",
    "    if not os.path.isfile(args.checkpoint):\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found at {args.checkpoint}\")\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4)\n",
    "    total_steps = len(train_dataloader) * args.epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_training_steps=total_steps,\n",
    "                    num_warmup_steps=args.num_warmup_steps)\n",
    "    model, optimizer, scheduler, start_epoch = load_checkpoint(model, optimizer, scheduler, args.checkpoint, device)\n",
    "    model.eval()\n",
    "\n",
    "    # Generate\n",
    "    generated_lyrics = generate_lyrics(\n",
    "        model=model,\n",
    "        midi_path=midi_path,\n",
    "        lyrics_tokenizer=lyrics_tokenizer,\n",
    "        midi_tokenizer=midi_tokenizer,\n",
    "        max_midi_length=args.max_midi_length,\n",
    "        max_lyrics_length=args.max_lyrics_length,\n",
    "        num_beams=args.num_beams,\n",
    "        input_text=args.input_text\n",
    "    )\n",
    "\n",
    "    print(\"\\nGenerated Lyrics:\")\n",
    "    print(generated_lyrics)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
